{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDdy-k2BVDAo"
      },
      "source": [
        "# *Colab Code Llama*\n",
        "---\n",
        "\n",
        "**Colab Code Llama**\n",
        "An Assistant built on Code Llama.\n",
        "\n",
        "**Colab Code Llama PRO**\n",
        "- Upload chats and documents.\n",
        "- Save chats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzGa3YSCgl4C"
      },
      "source": [
        "## Setup and Installation\n",
        "\n",
        "- You can run Colab Code Llama on a free Google Colab Notebook.\n",
        "- Save a copy of this notebook: Go to File -> Save a copy in Drive. (optional, but needed if you want to make changes).\n",
        "- Go to the menu -> Runtime -> Change Runtime Type - Select GPU (T4). Note that you'll have to limit context length to about 8k max for this. Or use an A100 40 GB for about 16k context length OR use an A100 80 GB for about 32k of context length. (for more compute, you can use Runpod [affiliate link here](https://tinyurl.com/yjxbdc9w)).\n",
        "- Then go to Runtime -> Run all.\n",
        "- It takes about 2 mins for the installation (which all happens in the cloud in this notebook).\n",
        "- Once all cells have run, you'll find the chat interface at the bottom.\n",
        "\n",
        "All of your data remains within your Google Drive and Google's computers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XROVA4cTJe9o",
        "outputId": "f66cf699-9d33-4fe8-a2cc-65503949ad68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a helpful pair-coding assistant.\n"
          ]
        }
      ],
      "source": [
        "# Set the SYSTEM PROMPT\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"You are a helpful pair-coding assistant.\"\"\"\n",
        "SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT\n",
        "\n",
        "print(SYSTEM_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2YZZel5hjLc",
        "outputId": "8c0ecda6-a55e-439e-d5e8-6b3530694d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter the context length (must be between 256 and 128000): 8000\n",
            "Using a context length of 8000 and a document length of 6000.\n"
          ]
        }
      ],
      "source": [
        "# Prompt user for context length\n",
        "while True:\n",
        "    user_input = input(\"Please enter the context length (must be between 256 and 128000): \")\n",
        "\n",
        "    try:\n",
        "        max_context = int(user_input)\n",
        "\n",
        "        if 256 <= max_context <= 128000:\n",
        "            break\n",
        "        else:\n",
        "            print(\"Context length must be between 256 and 128000. Please try again.\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "# Your existing code for setting other variables\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "max_doc_length = int(0.75 * max_context)  # max doc length is 75% of the context length\n",
        "max_doc_words = int(max_doc_length)\n",
        "\n",
        "print(f\"Using a context length of {max_context} and a document length of {max_doc_length}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-PAv_Oy1Pr2"
      },
      "source": [
        "### Google Drive Mounting (optional)\n",
        "- Allows you to download the model to Google Drive for faster startup next time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "oAzlPhWXVKXo",
        "outputId": "96e1a24d-80f1-42c3-fed4-4ddfe2d565c0"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f5129fa6b58e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWc_szlJP-Gx"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4qxY6lCflcf",
        "outputId": "80701c11-43dd-4ad8-8d07-3211d75e508b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-r3p5fpxy\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-r3p5fpxy\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 21dc5859421cf0d7d82d374b10f533611745a8c5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0.dev0)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.35.0.dev0)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.35.0.dev0)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0.dev0)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (2023.7.22)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.35.0.dev0-py3-none-any.whl size=7790405 sha256=abefb34d0f32351115cbeb3892b11bbffe0cdee77dca61f55ca1cbdaf3faacf3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s_vxxy2v/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Collecting auto-gptq\n",
            "  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.4.2%2Bcu118-cp310-cp310-linux_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.19.0 (from auto-gptq)\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from auto-gptq)\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.23.5)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.0.1+cu118)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.0)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.35.0.dev0)\n",
            "Collecting peft (from auto-gptq)\n",
            "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (17.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.14.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->auto-gptq)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (1.5.3)\n",
            "Collecting xxhash (from datasets->auto-gptq)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m967.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->auto-gptq)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.8.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
            "Installing collected packages: xxhash, rouge, dill, multiprocess, datasets, accelerate, peft, auto-gptq\n",
            "Successfully installed accelerate-0.23.0 auto-gptq-0.4.2+cu118 datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 peft-0.5.0 rouge-1.0.1 xxhash-3.4.1\n",
            "Collecting git+https://github.com/huggingface/optimum.git\n",
            "  Cloning https://github.com/huggingface/optimum.git to /tmp/pip-req-build-nzcj30me\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum.git /tmp/pip-req-build-nzcj30me\n",
            "  Resolved https://github.com/huggingface/optimum.git to commit c5ad7f971cb0a494eac03dc0909f146725f999c5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coloredlogs (from optimum==1.14.0.dev0)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum==1.14.0.dev0) (1.12)\n",
            "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum==1.14.0.dev0) (4.35.0.dev0)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum==1.14.0.dev0) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum==1.14.0.dev0) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum==1.14.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum==1.14.0.dev0) (0.17.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum==1.14.0.dev0) (2.14.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.14.0.dev0) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.14.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.14.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.14.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.14.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.14.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum==1.14.0.dev0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum==1.14.0.dev0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum==1.14.0.dev0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->optimum==1.14.0.dev0) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->optimum==1.14.0.dev0) (17.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.14.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.14.0.dev0) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.14.0.dev0) (0.4.0)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]>=4.26.0->optimum==1.14.0.dev0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.14.0.dev0) (3.20.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum==1.14.0.dev0)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.14.0.dev0) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.14.0.dev0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.14.0.dev0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.14.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.14.0.dev0) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.14.0.dev0) (3.8.6)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum==1.14.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.14.0.dev0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.14.0.dev0) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.14.0.dev0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.14.0.dev0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.14.0.dev0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.14.0.dev0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.14.0.dev0) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum==1.14.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum==1.14.0.dev0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum==1.14.0.dev0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->optimum==1.14.0.dev0) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum==1.14.0.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum==1.14.0.dev0) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum==1.14.0.dev0) (1.16.0)\n",
            "Building wheels for collected packages: optimum\n",
            "  Building wheel for optimum (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optimum: filename=optimum-1.14.0.dev0-py3-none-any.whl size=396804 sha256=61bd68a5e0db5a14533eaeff98436969488d884d453c6f1494a34f85cd6095d2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dw3ias1l/wheels/8f/fa/71/cd84bac993f8fa15de35a1c058b91b9394bbc638e56c4327fd\n",
            "Successfully built optimum\n",
            "Installing collected packages: sentencepiece, humanfriendly, coloredlogs, optimum\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.14.0.dev0 sentencepiece-0.1.99\n",
            "Collecting git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\n",
            "  Cloning https://github.com/HazyResearch/flash-attention.git to /tmp/pip-req-build-g9r4g3vg\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/HazyResearch/flash-attention.git /tmp/pip-req-build-g9r4g3vg\n",
            "  Resolved https://github.com/HazyResearch/flash-attention.git to commit 02ac572f3ffc4f402e4183aaa6824b45859d3ed3\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: rotary-emb\n",
            "  Building wheel for rotary-emb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rotary-emb: filename=rotary_emb-0.1-cp310-cp310-linux_x86_64.whl size=2743019 sha256=99694a0c6f94cf7275425c755a2b955e818c2ce5d8fef6c6d0a56d24c460ba33\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xwp4icde/wheels/6c/6e/f8/31d3bb2daa3898d876fd90e3bde72d4b0e852b3ef5d3d0a454\n",
            "Successfully built rotary-emb\n",
            "Installing collected packages: rotary-emb\n",
            "Successfully installed rotary-emb-0.1\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.3.2.tar.gz (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.0.1+cu118)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.2)\n",
            "Collecting ninja (from flash-attn)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.3.2-cp310-cp310-linux_x86_64.whl size=57037626 sha256=bdd150bed5326c8b042aef739d87a7dfbb45fa9395f4ad43756ea782524dd8d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/2a/7d/13ada26139195be2a5eb874c79d79d0d5afcc791bb08ca2052\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: ninja, einops, flash-attn\n",
            "Successfully installed einops-0.7.0 flash-attn-2.3.2 ninja-1.11.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install git+https://github.com/huggingface/transformers.git\n",
        "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "!pip3 install git+https://github.com/huggingface/optimum.git\n",
        "!pip3 install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\n",
        "!pip3 install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpbHJwz5jIRc",
        "outputId": "10e2dc20-4643-49c0-b94f-b6e9d29d05f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/5.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/5.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U pdfminer.six # could maybe add pre-built wheels to speed this up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8PdbAp1r5_B",
        "outputId": "3ec46722-c025-4df1-fe73-e82288a5ad5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wgbsa4U2TBk"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1zodOgcgqnO"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC87gzY8rtzN"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, TextStreamer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLhGWu9Ngvg"
      },
      "source": [
        "## Load Model\n",
        "If you have connected to Google Drive, the model will load from there (unless this is your first time connecting, in which case the model will be saved to Drive).\n",
        "- Takes about 2 mins first time around.\n",
        "- Takes about 1 min the 2nd time onwards with Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "dhmJnb4VrrIU",
        "outputId": "bc019593-39ed-4b35-8a9f-73431b026d85"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2732869391d6>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                                              cache_dir='')\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_bettertransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto_bettertransformer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3906\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0moptimum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbettertransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBetterTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3908\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBetterTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3910\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreverse_bettertransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/transformation.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(model, keep_original_model, max_memory, offload_dir, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mmodel_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_to_bettertransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mmodel_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_to_bettertransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/transformation.py\u001b[0m in \u001b[0;36mreplace_to_bettertransformer\u001b[0;34m(model, config)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mBetterTransformerManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCLUDE_FROM_TRANSFORM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             ):\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mreplace_to_bettertransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/transformation.py\u001b[0m in \u001b[0;36mreplace_to_bettertransformer\u001b[0;34m(model, config)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mBetterTransformerManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCLUDE_FROM_TRANSFORM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             ):\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mreplace_to_bettertransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/transformation.py\u001b[0m in \u001b[0;36mreplace_to_bettertransformer\u001b[0;34m(model, config)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mBetterTransformerManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCLUDE_FROM_TRANSFORM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             ):\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mreplace_to_bettertransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/transformation.py\u001b[0m in \u001b[0;36mreplace_to_bettertransformer\u001b[0;34m(model, config)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mshould_replace_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_replace_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 bettertransformer_module = BetterTransformerManager.MODEL_MAPPING[config.model_type][target_class](\n\u001b[0m\u001b[1;32m     99\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/models/decoder_models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layer, config)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"nn.Module\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"PretrainedConfig\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBetterTransformerBaseLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;34mf\" and `num_heads`: {self.num_heads}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             )\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"attribute_map\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LlamaConfig' object has no attribute 'attention_bias'"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# model_id = \"TheBloke/Yarn-Llama-2-7B-64K-GPTQ\"\n",
        "# model_id = \"TheBloke/Yarn-Llama-2-13B-128K-GPTQ\"\n",
        "# model_id = \"TheBloke/CodeLlama-13B-Instruct-GPTQ\"\n",
        "model_id = \"./drive/MyDrive/codellama_13b_gptq\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             device_map=\"auto\",\n",
        "                                             revision=\"main\",\n",
        "                                             trust_remote_code=True,\n",
        "                                             cache_dir='')\n",
        "\n",
        "model = model.to_bettertransformer()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysqp5fuyRWc4",
        "outputId": "898310d2-30bb-49ae-f838-23f91a362a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4096\n"
          ]
        }
      ],
      "source": [
        "print(model.config.max_position_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXgoG2ZvuHFI",
        "outputId": "e4ecaba3-c139-4df9-db58-10c41f5f2af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaConfig {\n",
            "  \"_name_or_path\": \"TheBloke/Llama-2-7B-chat-GPTQ\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_length\": 4096,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"batch_size\": 1,\n",
            "    \"bits\": 4,\n",
            "    \"block_name_to_quantize\": null,\n",
            "    \"damp_percent\": 0.01,\n",
            "    \"dataset\": null,\n",
            "    \"desc_act\": false,\n",
            "    \"disable_exllama\": false,\n",
            "    \"group_size\": 128,\n",
            "    \"model_seqlen\": null,\n",
            "    \"module_name_preceding_first_block\": null,\n",
            "    \"pad_token_id\": null,\n",
            "    \"quant_method\": \"gptq\",\n",
            "    \"sym\": true,\n",
            "    \"tokenizer\": null,\n",
            "    \"true_sequential\": true,\n",
            "    \"use_cuda_fp16\": false\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.32.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(model.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK4hxzkKoW3H"
      },
      "source": [
        "# Token calculator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvU6ntyzoWLl"
      },
      "outputs": [],
      "source": [
        "def count_tokens(text):\n",
        "    return len(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXGwnjsOoa1M"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOVCQfjYUgHN"
      },
      "source": [
        "## Set up the User Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLQlqxAIgFzF"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML, clear_output, Markdown\n",
        "import textwrap, json\n",
        "import ipywidgets as widgets\n",
        "import re, time\n",
        "from google.colab import files\n",
        "from pdfminer.high_level import extract_text\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7HZfpuDfLQU"
      },
      "outputs": [],
      "source": [
        "def generate_response(dialogs, temperature=0.01, top_p=0.9, logprobs=False):\n",
        "    torch.cuda.empty_cache()\n",
        "    # print(json.dumps(dialogs, indent=4))\n",
        "    max_prompt_len = int(0.85 * max_context)\n",
        "    max_gen_len = int(0.10 * max_prompt_len)\n",
        "\n",
        "    prompt_tokens = []\n",
        "    for dialog in dialogs:\n",
        "        if dialog[0][\"role\"] != \"system\":\n",
        "            dialog = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": SYSTEM_PROMPT,\n",
        "                }\n",
        "            ] + dialog\n",
        "        dialog_tokens = [tokenizer(\n",
        "            f\"{B_INST} {B_SYS}{(dialog[0]['content']).strip()}{E_SYS}{(dialog[1]['content']).strip()} {E_INST}\",\n",
        "            # f\"{B_INST} {(dialog[1]['content']).strip()} {E_INST}\", # Omits the system prompt altogether\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True\n",
        "        ).input_ids.to(\"cuda\")]\n",
        "        for i in range(2, len(dialog), 2):\n",
        "            user_tokens = tokenizer(\n",
        "                f\"{B_INST} {(dialog[i+1]['content']).strip()} {E_INST}\",\n",
        "                return_tensors=\"pt\",\n",
        "                add_special_tokens=True\n",
        "            ).input_ids.to(\"cuda\")\n",
        "            assistant_w_eos = dialog[i]['content'].strip() + tokenizer.eos_token\n",
        "            assistant_tokens = tokenizer(\n",
        "                            assistant_w_eos,\n",
        "                            return_tensors=\"pt\",\n",
        "                            add_special_tokens=False\n",
        "                        ).input_ids.to(\"cuda\")\n",
        "            tokens = torch.cat([assistant_tokens, user_tokens], dim=-1)\n",
        "            dialog_tokens.append(tokens)\n",
        "        prompt_tokens.append(torch.cat(dialog_tokens, dim=-1))\n",
        "\n",
        "    input_ids = prompt_tokens[0]\n",
        "    if len(input_ids[0]) > max_prompt_len:\n",
        "        return \"\\n\\n **The language model's input limit has been reached. Clear the chat and start afresh!**\"\n",
        "\n",
        "    # print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
        "\n",
        "    with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
        "      generation_output = model.generate(\n",
        "          input_ids=input_ids,\n",
        "          do_sample=True,\n",
        "          max_new_tokens=max_gen_len,\n",
        "          temperature=temperature,\n",
        "          top_p=top_p\n",
        "      );\n",
        "\n",
        "    new_tokens = generation_output[0][input_ids.shape[-1]:]\n",
        "    new_assistant_response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip();\n",
        "\n",
        "    return new_assistant_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_RJObixH_HR"
      },
      "outputs": [],
      "source": [
        "def print_wrapped(text):\n",
        "    # Regular expression pattern to detect code blocks\n",
        "    code_pattern = r'```(.+?)```'\n",
        "    matches = list(re.finditer(code_pattern, text, re.DOTALL))\n",
        "\n",
        "    if not matches:\n",
        "        # If there are no code blocks, display the entire text as Markdown\n",
        "        display(Markdown(text))\n",
        "        return\n",
        "\n",
        "    start = 0\n",
        "    for match in matches:\n",
        "        # Display the text before the code block as Markdown\n",
        "        before_code = text[start:match.start()].strip()\n",
        "        if before_code:\n",
        "            display(Markdown(before_code))\n",
        "\n",
        "        # Display the code block\n",
        "        code = match.group(0).strip()  # Extract code block\n",
        "        display(Markdown(code))  # Display code block\n",
        "\n",
        "        start = match.end()\n",
        "\n",
        "    # Display the text after the last code block as Markdown\n",
        "    after_code = text[start:].strip()  # Text after the last code block\n",
        "    if after_code:\n",
        "        display(Markdown(after_code))\n",
        "\n",
        "def grab_and_shorten_text(max_doc_length):\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "\n",
        "    # Check the file extension\n",
        "    if file_name.endswith('.txt'):\n",
        "        text = uploaded[file_name].decode()\n",
        "    elif file_name.endswith('.pdf'):\n",
        "        pdf_bytes = io.BytesIO(uploaded[file_name])\n",
        "        text = extract_text(pdf_bytes)\n",
        "    else:\n",
        "        raise ValueError('Unsupported file type. Please upload a .txt or .pdf file.')\n",
        "\n",
        "    with alert_out:\n",
        "        clear_output()  # Clear the previous alert\n",
        "        print(\"Shortening the text...\")\n",
        "\n",
        "    tokens = tokenizer.encode(text, truncation=True, max_length=max_doc_length, return_tensors='pt');\n",
        "    num_tokens_pdf = len(tokens)\n",
        "    print(f\"The PDF document has {num_tokens_pdf} tokens\")\n",
        "\n",
        "    num_tokens_system_prompt = count_tokens(SYSTEM_PROMPT)\n",
        "    print(f\"The system prompt has {num_tokens_system_prompt} tokens\")\n",
        "\n",
        "    # Count the number of total tokens\n",
        "    total_num_tokens = num_tokens_pdf + num_tokens_system_prompt\n",
        "    print(f\"The total number of tokens is {total_num_tokens}\")\n",
        "\n",
        "    shortened_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    return file_name, shortened_text\n",
        "\n",
        "dialog_history = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "\n",
        "button = widgets.Button(description=\"Send\")\n",
        "upload_button = widgets.Button(description=\"Upload .txt or .pdf\")\n",
        "text = widgets.Textarea(layout=widgets.Layout(width='800px'))\n",
        "\n",
        "output_log = widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    user_input = text.value\n",
        "    dialog_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    text.value = ''\n",
        "\n",
        "    # Change button description and color, and disable it\n",
        "    button.description = 'Processing...'\n",
        "    button.style.button_color = '#ff6e00'  # Use hex color codes for better color choices\n",
        "    button.disabled = True  # Disable the button when processing\n",
        "\n",
        "    with output_log:\n",
        "        clear_output()\n",
        "        for message in dialog_history:\n",
        "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
        "\n",
        "    assistant_response = generate_response([dialog_history]);\n",
        "\n",
        "    # Re-enable the button, reset description and color after processing\n",
        "    button.description = 'Send'\n",
        "    button.style.button_color = 'lightgray'\n",
        "    button.disabled = False\n",
        "\n",
        "    dialog_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "    with output_log:\n",
        "        clear_output()\n",
        "        for message in dialog_history:\n",
        "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "# Create an output widget for alerts\n",
        "alert_out = widgets.Output()\n",
        "\n",
        "# Define the 'Upload' button click event handler\n",
        "def on_upload_button_clicked(b):\n",
        "\n",
        "    file_name, uploaded_text = grab_and_shorten_text(max_doc_length)\n",
        "\n",
        "    with alert_out:\n",
        "        clear_output()  # Clear the previous alert\n",
        "        print(f\"Upload successful: {file_name}, processing the file...\")\n",
        "\n",
        "    user_input = f\"Uploaded document [{file_name}]: {uploaded_text}\"\n",
        "    dialog_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    time.sleep(0.1)  # slight delay to ensure order\n",
        "\n",
        "    assistant_input = f\"You have uploaded text from {file_name}\"\n",
        "    dialog_history.append({\"role\": \"assistant\", \"content\": assistant_input})\n",
        "\n",
        "    with output_log:\n",
        "        clear_output()\n",
        "        for message in dialog_history:\n",
        "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
        "\n",
        "    with alert_out:\n",
        "        clear_output()  # Clear the previous alert\n",
        "        # print(f\"File processing completed.\")\n",
        "\n",
        "upload_button.on_click(on_upload_button_clicked)\n",
        "\n",
        "clear_button = widgets.Button(description=\"Clear Chat\")\n",
        "text = widgets.Textarea(layout=widgets.Layout(width='800px'))\n",
        "\n",
        "def on_clear_button_clicked(b):\n",
        "    # Clear the dialog history\n",
        "    dialog_history.clear()\n",
        "    # Add back the initial system prompt\n",
        "    dialog_history.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
        "    # Clear the output log\n",
        "    with output_log:\n",
        "        clear_output()\n",
        "\n",
        "clear_button.on_click(on_clear_button_clicked)\n",
        "\n",
        "def save_chat(b):\n",
        "    # Serialize the chat history into a JSON string\n",
        "    chat_json = json.dumps(dialog_history)\n",
        "\n",
        "    # Write the chat history to a temporary file\n",
        "    with open('chat_history.json', 'w') as f:\n",
        "        f.write(chat_json)\n",
        "\n",
        "    # Download the file\n",
        "    files.download('chat_history.json')\n",
        "\n",
        "save_button = widgets.Button(description=\"Save Chat\")\n",
        "save_button.on_click(save_chat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZI8fCE7KERH"
      },
      "outputs": [],
      "source": [
        "# Define the function to upload chat\n",
        "def upload_chat(b):\n",
        "    # Upload the file\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Get the file name\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "\n",
        "    # Ensure the file is a .json file\n",
        "    if not file_name.endswith('.json'):\n",
        "        print('Error: Incorrect file type. Please upload a .json file.')\n",
        "        return\n",
        "\n",
        "    # Load the content of the file\n",
        "    chat_data = uploaded[file_name].decode()\n",
        "\n",
        "    # Load the JSON data from the file\n",
        "    try:\n",
        "        global dialog_history\n",
        "        dialog_history = json.loads(chat_data)\n",
        "    except json.JSONDecodeError:\n",
        "        print('Error: File is not in the correct format. Please upload a properly formatted .json file.')\n",
        "        return\n",
        "\n",
        "    with output_log:\n",
        "        clear_output()\n",
        "        for message in dialog_history:\n",
        "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
        "\n",
        "# Create the upload button and set the on_click event handler\n",
        "upload_chat_button = widgets.Button(description=\"Upload Chat\")\n",
        "upload_chat_button.on_click(upload_chat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy0iMfgLKKk2"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "from ipywidgets import HBox, VBox\n",
        "\n",
        "# Create the title with HTML\n",
        "title = f\"<h1 style='color: #ff6e00;'>Colab Code Llama 🦙 PRO</h1> <p>(Max context of: {max_context}. Uploaded files will be shortened to {max_doc_words} tokens)</p>\"\n",
        "\n",
        "# Assuming that output_log, alert_out, and text are other widgets or display elements...\n",
        "first_row = HBox([button, clear_button, upload_button])  # Arrange these buttons horizontally\n",
        "second_row = HBox([save_button, upload_chat_button])  # Arrange these buttons horizontally\n",
        "\n",
        "# Arrange the two rows of buttons and other display elements vertically\n",
        "layout = VBox([output_log, alert_out, text, first_row, second_row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiAEnqKuPGoR"
      },
      "source": [
        "# Chat with Colab Code Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263,
          "referenced_widgets": [
            "caaf096c98e44161acfaf3d37b5802a5",
            "887efd6e3edd4b748d29d49a77e38adb",
            "df0527ab8db44fe0a833228bf025012a",
            "775d16ffb6aa4896b62b274206c9323a",
            "bb00bbc5599f416ba10d4baf5ffde90b",
            "469844f2884246d895eeadb27f636d0f",
            "e04f400201de4d029635cffde5778bd0",
            "42e963a27167421eacab91f4f3fcaf20",
            "51d911c1a46444c08744f2b69657e138",
            "c5c9a9bebba04a54a9ce521fe202be60",
            "845c6f39f68d4ad5a5fa3aba8dd0e03f",
            "92e8eedb11024f199ef3ff3d574c96f2",
            "b40a5c8ef82a46ccb685ba0e173f6f82",
            "412aa177851245ffb57b8410eb4ec778",
            "e38d6029400448dc804ee2798182d608",
            "9727ddf7eb754449b71902765308b824",
            "e341c65e178a420e8b0802f1e48f63e7",
            "003ff5b92b2b4f2a951368779578f9c6",
            "722d344279244d0d8a2a1d2f74f22fcd",
            "fec48edcc96d46958a4257ab3af21b1d",
            "9559f640c1f04c2495a13d6a5fe6533d",
            "c127ad5b3f7d4f62b05763273b0b2657",
            "b9762738835e4b4ca62ca54c11e878e5",
            "a83c35d6bd19469abfc117a44d270443",
            "a5a8e3ea291f41c48564e8c98fd5e377",
            "1d183ce4c18e4014934e0e2077cb5c2e",
            "3526d273f9de4f1e9b8eae36d161ee87",
            "703f57b3b4ab42b0b0bd9cbd07ee234e"
          ]
        },
        "id": "_r5fQloTPGx5",
        "outputId": "8571ed7e-8827-4c16-e90b-8567e776812a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h1 style='color: #ff6e00;'>Colab Code Llama 🦙 PRO</h1> <p>(Max context of: 16000. Uploaded files will be shortened to 12000 tokens)</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "caaf096c98e44161acfaf3d37b5802a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Output(), Output(), Textarea(value='', layout=Layout(width='800px')), HBox(children=(Button(des…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e68366ba-9cb7-418b-a7c6-4d52063b5d8d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e68366ba-9cb7-418b-a7c6-4d52063b5d8d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving berkshire23.txt to berkshire23.txt\n"
          ]
        }
      ],
      "source": [
        "display(HTML(title))  # Use HTML function to display the title\n",
        "display(layout)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "X-PAv_Oy1Pr2"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "003ff5b92b2b4f2a951368779578f9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": "lightgray",
            "font_weight": ""
          }
        },
        "1d183ce4c18e4014934e0e2077cb5c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3526d273f9de4f1e9b8eae36d161ee87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "412aa177851245ffb57b8410eb4ec778": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Save Chat",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b9762738835e4b4ca62ca54c11e878e5",
            "style": "IPY_MODEL_a83c35d6bd19469abfc117a44d270443",
            "tooltip": ""
          }
        },
        "42e963a27167421eacab91f4f3fcaf20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "800px"
          }
        },
        "469844f2884246d895eeadb27f636d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_412aa177851245ffb57b8410eb4ec778",
              "IPY_MODEL_e38d6029400448dc804ee2798182d608"
            ],
            "layout": "IPY_MODEL_9727ddf7eb754449b71902765308b824"
          }
        },
        "51d911c1a46444c08744f2b69657e138": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "703f57b3b4ab42b0b0bd9cbd07ee234e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "722d344279244d0d8a2a1d2f74f22fcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "775d16ffb6aa4896b62b274206c9323a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextareaModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_42e963a27167421eacab91f4f3fcaf20",
            "placeholder": "​",
            "rows": null,
            "style": "IPY_MODEL_51d911c1a46444c08744f2b69657e138",
            "value": ""
          }
        },
        "845c6f39f68d4ad5a5fa3aba8dd0e03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Clear Chat",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_722d344279244d0d8a2a1d2f74f22fcd",
            "style": "IPY_MODEL_fec48edcc96d46958a4257ab3af21b1d",
            "tooltip": ""
          }
        },
        "887efd6e3edd4b748d29d49a77e38adb": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3526d273f9de4f1e9b8eae36d161ee87",
            "msg_id": "",
            "outputs": []
          }
        },
        "92e8eedb11024f199ef3ff3d574c96f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Upload .txt or .pdf",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_9559f640c1f04c2495a13d6a5fe6533d",
            "style": "IPY_MODEL_c127ad5b3f7d4f62b05763273b0b2657",
            "tooltip": ""
          }
        },
        "9559f640c1f04c2495a13d6a5fe6533d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9727ddf7eb754449b71902765308b824": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5a8e3ea291f41c48564e8c98fd5e377": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a83c35d6bd19469abfc117a44d270443": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b40a5c8ef82a46ccb685ba0e173f6f82": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9762738835e4b4ca62ca54c11e878e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb00bbc5599f416ba10d4baf5ffde90b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5c9a9bebba04a54a9ce521fe202be60",
              "IPY_MODEL_845c6f39f68d4ad5a5fa3aba8dd0e03f",
              "IPY_MODEL_92e8eedb11024f199ef3ff3d574c96f2"
            ],
            "layout": "IPY_MODEL_b40a5c8ef82a46ccb685ba0e173f6f82"
          }
        },
        "c127ad5b3f7d4f62b05763273b0b2657": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c5c9a9bebba04a54a9ce521fe202be60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Send",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e341c65e178a420e8b0802f1e48f63e7",
            "style": "IPY_MODEL_003ff5b92b2b4f2a951368779578f9c6",
            "tooltip": ""
          }
        },
        "caaf096c98e44161acfaf3d37b5802a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_887efd6e3edd4b748d29d49a77e38adb",
              "IPY_MODEL_df0527ab8db44fe0a833228bf025012a",
              "IPY_MODEL_775d16ffb6aa4896b62b274206c9323a",
              "IPY_MODEL_bb00bbc5599f416ba10d4baf5ffde90b",
              "IPY_MODEL_469844f2884246d895eeadb27f636d0f"
            ],
            "layout": "IPY_MODEL_e04f400201de4d029635cffde5778bd0"
          }
        },
        "df0527ab8db44fe0a833228bf025012a": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_703f57b3b4ab42b0b0bd9cbd07ee234e",
            "msg_id": "",
            "outputs": []
          }
        },
        "e04f400201de4d029635cffde5778bd0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e341c65e178a420e8b0802f1e48f63e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38d6029400448dc804ee2798182d608": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Upload Chat",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_a5a8e3ea291f41c48564e8c98fd5e377",
            "style": "IPY_MODEL_1d183ce4c18e4014934e0e2077cb5c2e",
            "tooltip": ""
          }
        },
        "fec48edcc96d46958a4257ab3af21b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
